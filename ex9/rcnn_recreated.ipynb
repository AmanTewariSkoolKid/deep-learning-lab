{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fa32f8c",
   "metadata": {},
   "source": [
    "# R-CNN minimal \n",
    "This notebook is a compact, well-commented re-creation of the `Object Detection using RCNN` notebook you provided. It is adapted to use the local `ex 9/data` layout and the filtered/pixel-annotated CSV created by the other notebook (`ex 9/data/interim/annotations_oi_filtered_pixels.csv`).\n",
    "\n",
    "Notes for laymen: each code cell has clear comments. Run cells in order. If a cell fails because of missing packages (e.g., `selectivesearch`), follow the short install instructions in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dc3f69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for pixel-annotated CSV at: C:\\Users\\amant\\Documents\\aaa-COLLEGE\\aaa-semester 5\\deep learning lab\\ex9\\ex 9\\data\\interim\\annotations_oi_filtered_pixels.csv\n",
      "Images dir: C:\\Users\\amant\\Documents\\aaa-COLLEGE\\aaa-semester 5\\deep learning lab\\ex9\\ex 9\\data\\images\n"
     ]
    }
   ],
   "source": [
    "# If you need to install dependencies, uncomment and run the following line (one-time):\n",
    "# !pip install selectivesearch torch torchvision pandas pillow matplotlib scikit-learn\n",
    "\n",
    "# Lightweight imports used by the rest of the notebook\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import selectivesearch\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths (uses the data you have under ex 9/data)\n",
    "ROOT = Path('.').resolve()\n",
    "EX9 = ROOT / 'ex 9'\n",
    "DATA = EX9 / 'data'\n",
    "INTERIM = DATA / 'interim'\n",
    "PIXELS_CSV = INTERIM / 'annotations_oi_filtered_pixels.csv'\n",
    "IM_DIR = DATA / 'images'\n",
    "\n",
    "print('Looking for pixel-annotated CSV at:', PIXELS_CSV)\n",
    "print('Images dir:', IM_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a23d0c82",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 9) (448139515.py, line 9)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mUnique images in filtered CSV:', ann['ImageID'].nunique())\u001b[39m\n                                                 ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 9)\n"
     ]
    }
   ],
   "source": [
    "# Load the pixel-annotated annotations that the other notebook creates.\n",
    "# The CSV is expected to contain at least: ImageID, LabelName, XMinPx, XMaxPx, YMinPx, YMaxPx\n",
    "if not PIXELS_CSV.exists():\n",
    "    print('Pixel CSV not found at', PIXELS_CSV)\n",
    "    print('Run the previous notebook to create:', PIXELS_CSV)\n",
    "else:\n",
    "    ann = pd.read_csv(PIXELS_CSV)\n",
    "    display(ann.head())\n",
    "Unique images in filtered CSV:', ann['ImageID'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2df175",
   "metadata": {},
   "source": [
    "## Helper functions (IoU, selective search wrapper, image loader)\n",
    "These helpers are small and explained for clarity. IoU measures overlap between two boxes. Selective search returns candidate boxes; we filter and convert them to the x1,y1,x2,y2 form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407e6dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(boxA, boxB, eps=1e-6):\n",
    "    # boxes are (x1,y1,x2,y2)\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    interW = max(0, xB - xA)\n",
    "    interH = max(0, yB - yA)\n",
    "    inter = interW * interH\n",
    "    areaA = max(0, (boxA[2]-boxA[0])) * max(0, (boxA[3]-boxA[1]))\n",
    "    areaB = max(0, (boxB[2]-boxB[0])) * max(0, (boxB[3]-boxB[1]))\n",
    "    union = areaA + areaB - inter + eps\n",
    "    return inter / union\n",
    "\n",
    "def extract_selective_search_candidates(image, scale=200, min_size=100, max_candidates=500):\n",
    "    # image: numpy RGB image\n",
    "    img_lbl, regions = selectivesearch.selective_search(image, scale=scale, min_size=min_size)\n",
    "    candidates = set()\n",
    "    img_area = image.shape[0] * image.shape[1]\n",
    "    for r in regions:\n",
    "        x, y, w, h = r['rect']\n",
    "        if w == 0 or h == 0:\n",
    "            continue\n",
    "        if r.get('size', 0) < 0.001 * img_area:\n",
    "            continue\n",
    "        # convert to x1,y1,x2,y2 and ensure in-bounds\n",
    "        x1 = max(0, x)\n",
    "        y1 = max(0, y)\n",
    "        x2 = min(image.shape[1], x + w)\n",
    "        y2 = min(image.shape[0], y + h)\n",
    "        candidates.add((x1, y1, x2, y2))\n",
    "        if len(candidates) >= max_candidates:\n",
    "            break\n",
    "    return list(candidates)\n",
    "\n",
    "def load_image_for_id(image_id, images_dir=IM_DIR):\n",
    "    # robust lookup: common extensions and recursive search\n",
    "    p = Path(images_dir) / f'{image_id}.jpg'\n",
    "    if not p.exists():\n",
    "        p = Path(images_dir) / f'{image_id}.jpeg'\n",
    "    if not p.exists():\n",
    "        p = Path(images_dir) / f'{image_id}.png'\n",
    "    if not p.exists():\n",
    "        candidates = list(Path(images_dir).rglob(f'{image_id}.*'))\n",
    "        p = candidates[0] if candidates else None\n",
    "    if p is None or not p.exists():\n",
    "        return None\n",
    "    img = cv2.imread(str(p), cv2.IMREAD_COLOR)[..., ::-1]  # BGR->RGB\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2aa84a",
   "metadata": {},
   "source": [
    "## Build a small R-CNN-style proposal dataset (safe demo)\n",
    "We will: pick up to N images, generate proposals, match proposals to ground-truth boxes using IoU, label proposals (object class or background), and compute the simple bbox delta encoding used as regression targets.\n",
    "This step can be slow if you run many images; it is safe to run with N=50 or N=100 for a CPU demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dba0ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_delta(gt_box, prop_box, image_wh):\n",
    "    # simple delta: (dx, dy, dw, dh) normalized by image size\n",
    "    gx1, gy1, gx2, gy2 = gt_box\n",
    "    px1, py1, px2, py2 = prop_box\n",
    "    W, H = image_wh\n",
    "    dx = (gx1 - px1) / max(1, W)\n",
    "    dy = (gy1 - py1) / max(1, H)\n",
    "    dw = (gx2 - px2) / max(1, W)\n",
    "    dh = (gy2 - py2) / max(1, H)\n",
    "    return [dx, dy, dw, dh]\n",
    "\n",
    "def build_proposal_dataset(pixels_csv=PIXELS_CSV, N_images=80, iou_threshold_pos=0.5, max_props_per_image=300):\n",
    "    if not Path(pixels_csv).exists():\n",
    "        raise FileNotFoundError(f'Pixel CSV not found: {pixels_csv}')\n",
    "    ann = pd.read_csv(pixels_csv)\n",
    "    image_ids = ann['ImageID'].unique().tolist()\n",
    "    random.shuffle(image_ids)\n",
    "    image_ids = image_ids[:N_images]\n",
    "    dataset = []\n",
    "    for img_id in image_ids:\n",
    "        img = load_image_for_id(img_id, IM_DIR)\n",
    "        if img is None:\n",
    "            print('Image not found for', img_id); continue\n",
    "        H, W = img.shape[:2]\n",
    "        df = ann[ann['ImageID'] == img_id]\n",
    "        # ground-truth boxes for this image as x1,y1,x2,y2 (integers)\n",
    "        gt_boxes = df[['XMinPx','YMinPx','XMaxPx','YMaxPx']].values.tolist()\n",
    "        gt_labels = df['LabelName'].tolist()\n",
    "        props = extract_selective_search_candidates(img, max_candidates=max_props_per_image)\n",
    "        # for each prop find best IoU with any GT box\n",
    "        for p in props:\n",
    "            best_iou = 0.0\n",
    "            best_idx = -1\n",
    "            for gi, g in enumerate(gt_boxes):\n",
    "                score = iou(p, g)\n",
    "                if score > best_iou:\n",
    "                    best_iou = score\n",
    "                    best_idx = gi\n",
    "            if best_iou >= iou_threshold_pos and best_idx >= 0:\n",
    "                label = gt_labels[best_idx]  # MID; we keep MID for mapping later\n",
    "                delta = compute_delta(gt_boxes[best_idx], p, (W,H))\n",
    "            else:\n",
    "                label = 'background'\n",
    "                delta = [0,0,0,0]\n",
    "            dataset.append({'image_id': img_id, 'prop': p, 'label': label, 'delta': delta, 'image_wh': (W,H)})\n",
    "    print('Built proposal dataset with', len(dataset), 'proposals from', len(image_ids), 'images')\n",
    "    return dataset\n",
    "\n",
    "# Build a small dataset (change N_images to a smaller number if your CPU is slow)\n",
    "small_ds = build_proposal_dataset(N_images=40, iou_threshold_pos=0.5, max_props_per_image=250)\n",
    "\n",
    "# show a few examples to sanity check\n",
    "for ex in small_ds[:4]:\n",
    "    img = load_image_for_id(ex['image_id'], IM_DIR)\n",
    "    x1,y1,x2,y2 = ex['prop']\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.imshow(img)\n",
    "    plt.gca().add_patch(plt.Rectangle((x1,y1), x2-x1, y2-y1, edgecolor='r', facecolor='none', linewidth=2))\n",
    "    plt.title(ex['label'])\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cc94ce",
   "metadata": {},
   "source": [
    "## Small R-CNN training skeleton (demo)\n",
    "We use a pretrained VGG backbone for feature extraction on proposal crops, then train a small linear head for classification and a small regression head for bbox deltas. The training below is a *demo* and keeps things simple so it runs on CPU for small datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774f4648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build label mapping (keep 'background' as index 0)\n",
    "labels = sorted(set([d['label'] for d in small_ds]))\n",
    "if 'background' not in labels:\n",
    "    labels = ['background'] + labels\n",
    "label2idx = {l:i for i,l in enumerate(labels)}\n",
    "print('Labels:', labels)\n",
    "\n",
    "# Simple dataset wrapper that returns crops, class idx and deltas\n",
    "class RCNNProposalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, proposals, transform=None, image_dir=IM_DIR):\n",
    "        self.proposals = proposals\n",
    "        self.transform = transform or transforms.Compose([transforms.ToTensor(), transforms.Resize((224,224))])\n",
    "        self.image_dir = image_dir\n",
    "    def __len__(self):\n",
    "        return len(self.proposals)\n",
    "    def __getitem__(self, idx):\n",
    "        rec = self.proposals[idx]\n",
    "        img = load_image_for_id(rec['image_id'], self.image_dir)\n",
    "        x1,y1,x2,y2 = rec['prop']\n",
    "        crop = img[y1:y2, x1:x2]\n",
    "        if crop.size == 0:\n",
    "            # fallback: return a zero tensor\n",
    "            crop = np.zeros((224,224,3), dtype=np.uint8)\n",
    "        # to PIL for torchvision transforms\n",
    "        crop = Image.fromarray(crop)\n",
    "        x = self.transform(crop)\n",
    "        label = label2idx.get(rec['label'], 0)\n",
    "        delta = torch.tensor(rec['delta'], dtype=torch.float32)\n",
    "        return x, label, delta\n",
    "\n",
    "# create dataset and dataloader (small batch size)\n",
    "torch.manual_seed(0)\n",
    "dataset = RCNNProposalDataset(small_ds)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Build a tiny model: pretrained VGG features + heads\n",
    "backbone = models.vgg16(pretrained=True)\n",
    "# remove classifier and use features + adaptive pool to flatten\n",
    "backbone.classifier = torch.nn.Identity()\n",
    "for p in backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "backbone = backbone.to(device).eval()\n",
    "\n",
    "class SmallRCNN(torch.nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone.features\n",
    "        self.pool = torch.nn.AdaptiveAvgPool2d((7,7))\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.fc_cls = torch.nn.Linear(25088, n_classes)\n",
    "        self.fc_reg = torch.nn.Linear(25088, 4)\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        cls = self.fc_cls(x)\n",
    "        reg = self.fc_reg(x)\n",
    "        return cls, reg\n",
    "\n",
    "model = SmallRCNN(len(labels)).to(device)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "cls_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "reg_loss_fn = torch.nn.L1Loss()\n",
    "\n",
    "# Demo training loop (single epoch, small dataset)\n",
    "model.train()\n",
    "for xb, yb, db in loader:\n",
    "    xb = xb.to(device)\n",
    "    yb = yb.to(device)\n",
    "    db = db.to(device)\n",
    "    opt.zero_grad()\n",
    "    cls_pred, reg_pred = model(xb)\n",
    "    loss_cls = cls_loss_fn(cls_pred, yb)\n",
    "    # only compute reg loss on non-background examples\n",
    "    mask = (yb != label2idx['background'])\n",
    "    if mask.any():\n",
    "        loss_reg = reg_loss_fn(reg_pred[mask], db[mask])\n",
    "    else:\n",
    "        loss_reg = torch.tensor(0., device=device)\n",
    "    loss = loss_cls + 5.0 * loss_reg\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    print('Batch loss:', float(loss.detach().cpu().numpy()))\n",
    "    break  # demo: run only one batch to show it works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc007eea",
   "metadata": {},
   "source": [
    "## Next steps and notes\n",
    "- The above cells give a working, small R-CNN style pipeline for *learning* purposes: data prep -> proposals -> labeling -> tiny training.\n",
    "- For a full experiment: save `small_ds` to disk, implement full trainer with epochs/checkpoints, add evaluation (mAP) and faster RoI pooling for Fast R-CNN.\n",
    "\n",
    "If you'd like, I can now: (A) wire the helper functions into a CLI script `scripts/prepare_openimages.py` to automate the data prep, (B) expand training to a full trainer with checkpointing and evaluation, or (C) add a small unit test for IoU and delta computation. Tell me which and I'll implement it next."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
