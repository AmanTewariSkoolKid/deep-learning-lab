Experiment 4: Training Deep Networks (Loss, Backpropagation & Optimization)
a.	Implement and visualize Activation Functions: Sigmoid, ReLU, Tanh, Softmax
b.	Implement and visualize Loss Functions: MSE, Cross-Entropy (generate plots for behavioral understanding)
c.	Implement Backpropagation for training the network
d.	Compare Optimizers (SGD, Momentum, Adam) on a small dataset to analyze convergence speed and performance
